{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "1. Backpropagation\n",
    "    - Wiederholung Feed Forward Neural Network mit Parametern $M_1,M_2,...,M_k$\n",
    "    - Generalisierte Darstellung\n",
    "    - Backpropagation-Algorithmus\n",
    "2. K-means Clustering\n",
    "    - Motivation\n",
    "    - Minimierungsproblem\n",
    "    - Anwendungsfälle\n",
    "# Backpropagation:\n",
    "## Wiederholung\n",
    "Zur Regression bzw. Klassifizierung verwenden wir nun parametrisierbare Basiselemente und fügen diese durch Summation und Komposition zu einem Netzwerk zusammen, dessen Parameter wir auf den Zielvektor $y\\in\\mathbb{R}^N$ hin optimieren möchten.\n",
    "### Definition: Feed Forward Neural Network mit Parametern $M_1, M_2, ...,M_k$\n",
    "#### Voraussetzungen:\n",
    "<br>\n",
    "<div style=\"border:1px solid lightgrey;padding:20px;\">\n",
    "Daten $\\{x_p,y_p\\}_{p=1}^P$ mit $x_p\\in\\mathbb{R}^N,y\\in\\mathbb{R}$<br><br>\n",
    "$M_1,M_2,\\dots,M_k$, wobei $M_i$ die Anzahl der Neuronen des $i$-ten Hidden Layers angibt<br><br>\n",
    "$k$ bezeichnet die Gesamtanzahl an Hidden Layern <br><br>\n",
    "$f:\\mathbb{R}^N\\rightarrow \\mathbb{R}$ eine Basisfunktion <br>(z.b. $f(x)=tanh(c_m+x^Tv_m)$ mit Parametern $c_m\\in\\mathbb{R}, v_m\\in\\mathbb{R}^N$)<br><br>\n",
    "Aktivierungsfunktion $a:\\mathbb{R}\\rightarrow\\mathbb{R}$ \n",
    "<br>(z.b. $a(\\centerdot)=tanh(\\centerdot)$)\n",
    "</div>\n",
    "<br><br>\n",
    "Folgendes Bild zeigt die Konstruktion von Basisfeatures $f_m$, mit Hilfe derer man $r=\\sum_{m=1}^{M}w_mf_m$ konstruiert: <br><br>\n",
    "<img src=\"FeedForwardNeuralNetwork.jpg\">\n",
    "\n",
    "## Über die Konstruktion eines Basiselements $f_m$ zur Zielfunktion $r$ \n",
    "Sei $m\\in\\{1,\\dots,M\\}$ beliebig aber fest.<br>\n",
    "Es seien $c^{(2)}=(c_{1}^{(2)},\\dots,c_{m_2}^{(2)},\\dots,c_{M}^{(2)})^T\\in\\mathbb{R}^{M_2\\times1}$ sowie <br>$c^{(1)}=(c_{1}^{(1)},\\dots,c_{m}^{(1)},\\dots,c_{M_1}^{(1)})^T\\in\\mathbb{R}^{M_1\\times1}$ und <br>\n",
    "$V=???$\n",
    "<br><br>\n",
    "Dann kann man schreiben:<br>\n",
    "$f_m(x_p)=a(c_m^{(1)}+\\sum_{m_2=1}^{M_2}[a(c_{m_2}^{(2)}+\\sum_{n=1}^Nx_{p,n}v_{n,m_2}^{(2)})v_{m_2,m}^{(1)}]$\n",
    "<br><br>\n",
    "Und noch kompakter in Matrixschreibweise für alle $m=1,\\dots,M_1$:<br>\n",
    "$ f_p=a(c^{(1)}+V^{T(1)}a(c^{(2)}+V^{T(2)}x_p))$, wobei $a(\\centerdot)$ komponentenweise operiert.\n",
    "<br><br>\n",
    "Somit erhält man für $r=\\sum_{m=1}^{M_1}w_mf_m=b+w^Ta[c^{(1)}+V^{T(1)}a(c^{(2)}+V^{T(2)}x_p)]$ das Rekursionsmuster<br><br>\n",
    "0. $r^{(0)}=b+w^Ta(r^{(1)})$\n",
    "1. $r^{(1)}=c^{(1)}+V^{T(1)}a(r^{(2)})$\n",
    "2. $r^{(2)}=c^{(2)}+V^{T(2)}x_p$\n",
    "<br><br>\n",
    "Im Folgenden werden wir dieses Rekursionsmuster in Kombination mit einer Kostenfunktion $h(\\centerdot)$ ausnutzen, um den Gradienten für das Gradientenverfahren angeben zu können. Dieses Vorgehen bezeichnet man als Backpropagation-Algorithmus:\n",
    "## Backpropagation-Algorithmus\n",
    "Die Zielsetzung besteht darin, eine Kostenfunktion $h(r)$ mit dem Gradientenverfahren minimieren zu können. Dabei ist folgende Verkettungsfolge gegeben:<br>\n",
    "$h(r)\\rightarrow r(b,w) \\rightarrow [b(c^{(1)},V^{(1)}),w(c^{(1)},V^{(1)})]\\rightarrow [c^{(1)}(c^{(2)},V^{(2)}),V^{(1)}(c^{(2)},V^{(2)})]\\rightarrow[c^{(2)}(x_p),V^{(2)}(x_p)]$\n",
    "\n",
    "<br><br><br><br>\n",
    "Übliche Funktionsscharen für die Basiselemente sind<br>\n",
    "1. $b_m(x)=tanh(c_m+x_mv_m)$ mit Aktivierungsfunktion $a(.)=tanh(.)$<br>\n",
    "2. $b_m(x)=max(0,c_m+x^Tv_m)$ mit Aktivierungsfunktion $a(.)=max(0,.)$\n",
    "\n",
    "<br><br><br>\n",
    "#### Konstruktion eines  $(k,(M_1,M_2,...,M_n))$ - feed forward Neural Networks\n",
    "Recap: Es geht darum, eine geeignete Funktion $f = (f_1,f_2,...,f_N):\\mathbb{R}^N\\rightarrow\\mathbb{R}$ zu finden. Wir möchten nun auf Basis der Basiselemente $b_m$ Basis-Features (eines für jedes $F_i$) erzeugen, deren Konstruktion durch $k$ und $(M_1,M_2,...,M_n)$ festgelegt wird. Es wird nach folgendem Schema konstruiert:\n",
    "\n",
    "$f_m(x) = a( \\sum_{m_1=1}^{M_1}b_1(\\sum_{m_2=1}^{M_2}b_2(\\sum_{m_3=1}^{M_3}b_3(\\sum_{m_4=1}^{M_4}(\\dots( \\dots \\sum_{m_k=1}^{M_k}b_k))))\\dots)$<br>\n",
    "und das wäre beispielsweise für $b_m(x)=tanh(c_m+x^Tv_m)$ mit Aktivierungsfunktion $a(.)=tanh(.)$:<br>\n",
    "$f_m(x) = tanh( \\sum_{m_1=1}^{M_1}c_{m_1}+(\\sum_{m_2=1}^{M_2}a(\\sum_{m_3=1}^{M_3}b_2(\\sum_{m_4=1}^{M_4}\\dots)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
