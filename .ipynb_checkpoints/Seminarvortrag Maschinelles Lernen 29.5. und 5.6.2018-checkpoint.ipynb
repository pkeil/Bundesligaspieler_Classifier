{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inhalt\n",
    "1. Backpropagation\n",
    "    - Wiederholung Feed Forward Neural Network mit Parametern $M_1,M_2,...,M_k$\n",
    "    - Generalisierte Darstellung\n",
    "    - Backpropagation-Algorithmus\n",
    "    - Optimierungszyklen\n",
    "2. Fixed Feature Kernels\n",
    "    - Fundamentales Theorem der Linearen Algebra\n",
    "    - Anwendung auf die Kostenfunktion\n",
    "3. PCA und K-Means Clustering\n",
    "    - Motivation & Mehrwert\n",
    "4. K-Means Clustering\n",
    "    - Ziel und grundlegende Defintionen\n",
    "    - Von der Problemstellung zum Optimierungsproblem\n",
    "    - Der K-Means-Algorithmus\n",
    "    - Verbesserung des K-Means-Algorithmus'\n",
    "5. Primary Component Analysis (PCA)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Backpropagation\n",
    "## Wiederholung Feed Forward Neural Network\n",
    "Zur Regression bzw. Klassifizierung verwenden wir nun parametrisierbare Basiselemente und fügen diese durch Summation und Komposition zu einem Netzwerk zusammen, dessen Parameter wir auf den Zielvektor $y\\in\\mathbb{R}^N$ hin optimieren möchten.\n",
    "### Definition: Feed Forward Neural Network mit Parametern $M_1, M_2, ...,M_k$\n",
    "#### Voraussetzungen:\n",
    "<br>\n",
    "<div style=\"border:1px solid lightgrey;padding:20px;\">\n",
    "Daten $\\{x_p,y_p\\}_{p=1}^P$ mit $x_p\\in\\mathbb{R}^N,y\\in\\mathbb{R}$<br><br>\n",
    "$M_1,M_2,\\dots,M_k$, wobei $M_i$ die Anzahl der Neuronen des $i$-ten Hidden Layers angibt<br><br>\n",
    "$k$ bezeichnet die Gesamtanzahl an Hidden Layern <br><br>\n",
    "$f:\\mathbb{R}^N\\rightarrow \\mathbb{R}$ eine Basisfunktion <br>(z.b. $f(x)=tanh(c_m+x^Tv_m)$ mit Parametern $c_m\\in\\mathbb{R}, v_m\\in\\mathbb{R}^N$)<br><br>\n",
    "Aktivierungsfunktion $a:\\mathbb{R}\\rightarrow\\mathbb{R}$ \n",
    "<br>(z.b. $a(\\centerdot)=tanh(\\centerdot)$)\n",
    "</div>\n",
    "<br><br>\n",
    "Folgendes Bild zeigt die Konstruktion von Basisfeatures $f_m$, mit Hilfe derer man $r=\\sum_{m=1}^{M}w_mf_m$ konstruiert: <br><br>\n",
    "<img src=\"FeedForwardNeuralNetwork.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Über die Konstruktion eines Basiselements $f_m$ zur Zielfunktion $r$ \n",
    "Sei $m\\in\\{1,\\dots,M\\}$ beliebig aber fest.<br>\n",
    "Es seien $c^{(2)}=(c_{1}^{(2)},\\dots,c_{m_2}^{(2)},\\dots,c_{M}^{(2)})^T\\in\\mathbb{R}^{M_2\\times1}$ sowie <br>$c^{(1)}=(c_{1}^{(1)},\\dots,c_{m}^{(1)},\\dots,c_{M_1}^{(1)})^T\\in\\mathbb{R}^{M_1\\times1}$ und <br>\n",
    "$V\\in\\mathbb{R}^{M_2\\times M_1}$\n",
    "<br><br>\n",
    "Dann kann man schreiben:<br>\n",
    "$$f_m(x_p)=a(c_m^{(1)}+\\sum_{m_2=1}^{M_2}[a(c_{m_2}^{(2)}+\\sum_{n=1}^Nx_{p,n}v_{n,m_2}^{(2)})v_{m_2,m}^{(1)}]$$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalisierte Darstellung\n",
    "Und noch kompakter in Matrixschreibweise für alle $m=1,\\dots,M_1$:<br><br>\n",
    "$$ f_p=a(c^{(1)}+V^{T(1)}a(c^{(2)}+V^{T(2)}x_p)),$$\n",
    "<br><br>\n",
    "wobei $a(\\centerdot)$ nun komponentenweise operiert.\n",
    "<br><br><br>\n",
    "Somit erhält man für $r=\\sum_{m=1}^{M_1}w_mf_m=b+w^Ta[c^{(1)}+V^{T(1)}a(c^{(2)}+V^{T(2)}x_p)]$ das Rekursionsmuster<br><br>\n",
    "0. $r^{(0)}=b+w^Ta(r^{(1)})$\n",
    "1. $r^{(1)}=c^{(1)}+V^{T(1)}a(r^{(2)})$\n",
    "2. $r^{(2)}=c^{(2)}+V^{T(2)}x_p$\n",
    "<br><br>\n",
    "Im Folgenden werden wir dieses Rekursionsmuster in Kombination mit einer Kostenfunktion $h(\\centerdot)$ ausnutzen, um den Gradienten für das Gradientenverfahren angeben zu können. Dieses Vorgehen bezeichnet man als Backpropagation-Algorithmus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Backpropagation-Algorithmus\n",
    "Die Zielsetzung besteht darin, eine Kostenfunktion $h(r)$ mit dem Gradientenverfahren minimieren zu können. Dabei ist folgende Verkettungsfolge gegeben:<br>\n",
    "$h(r)\\rightarrow r(b,w) \\rightarrow [b(c^{(1)},V^{(1)}),w(c^{(1)},V^{(1)})]\\rightarrow [c^{(1)}(c^{(2)},V^{(2)}),V^{(1)}(c^{(2)},V^{(2)})]\\rightarrow[c^{(2)}(x_p),V^{(2)}(x_p)]$\n",
    "<br><br>\n",
    "<p>Die explizite Berechnung für eine allgemeine Kostenfunktion h$(\\centerdot)$ ist der Seite 204 in Machine Learning Refined zu entnehmen.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimierungszyklen\n",
    "Daraus erigbt sich folgendes Optimierungsschema:<br><br>\n",
    "$\\rightarrow$ Berechnung des Fehlers $h(r)$ für initiale (später verbesserte) Parametrisierung des Modells per \"Durchreichen\" des Inputs (Feed Forward)<br>\n",
    "$\\rightarrow$ Rückwärtsermittlung des Gradienten mit der Kettenregel (Backpropagation)<br>\n",
    "$\\rightarrow$ Verbesserung der Modellparameter aus $C$ und $V$<br>\n",
    "$\\rightarrow$ Feed Forward<br>\n",
    "$\\rightarrow$ Backpropagation<br>\n",
    "$\\rightarrow$ Anpassung Modellparameter<br>\n",
    "$\\rightarrow$ usw.<br>\n",
    "<br>\n",
    "Wir nennen einen Optimierungszyklus aus dem Feed-Forward-Schritt, der Backpropagation und der Verbesserung der Modellparameter eine <b>Epoche</b>.<br><br>\n",
    "<img src=\"backpropagation.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fixed Feature Kernel\n",
    "Motivation: Wir suchen eine andere Darstellung von unserer Feature-Matrix $F$ um dessen Speicherung zu umgehen, da diese in hohen Dimensionen nicht effektiv wäre.<br>\n",
    "## Kurzer Exkurs: Fundamentales Theorem der Linearen Algebra\n",
    "Jeder Vektor $w \\in \\mathbb{R}^M$ kann durch Linearkombination in einem Unterraum, der durch eine Reihe von $M$-dimensionalen Vektoren $\\{ f_p \\}_{p=1}^{P}$ ($P\\leq M$) aufgespannt wird,und einem dazu orthogonalen Vektor dargestellt werden.<br>\n",
    "     $$w= \\displaystyle \\sum_{p=1}^{P} f_p z_p +r$$ <br>\n",
    "     \n",
    "   Hierbei sind die $z_p$ die zu $w$ zugehörigen Gewichte und $r\\in \\mathbb{R}^{M}$ . Oder einfacher: \n",
    "       $$w=Fz+r$$,<br>\n",
    "       wobei wir die $f_p$ spaltenweise in eine $M\\times P$ Matrix $F$ speichern und die $z_p$ in einen Vektor $z\\in \\mathbb{R}^{P}$. Das nachfolgende Bild veranschaulicht dies noch einmal.<br>\n",
    "   <img src=FToLA.png> <br>\n",
    "## Anwendung auf die Kostenfunktion\n",
    "Wir ersetzen nun die Gewichte der Kostenfunktionen mit $w=Fz+r$. Exemplarisch für die kleinste Quadrate Kostenfunktion:\n",
    "$$g(b,w)=\\sum_{p=1}^P (b+f_p^Tw-y_p)^2$$\n",
    "$$\\iff g(b,z)=\\sum_{p=1}^P (b+f_p^T(Fz+r)-y_p)^2=\\sum_{p=1}^P (b+f_p^T Fz-y_p)^2$$\n",
    "Es entsteht die symmetrische Matrix $H=F^T F$ wobei $h_p=F^T f_p$ die $p$te Spalte ist. Also: \n",
    "$$g(b,z)=\\sum_{p=1}^P (b+h_p^T z-y_p)^2$$\n",
    "Für andere Kostenfunktionen analog. Wir können nun die Kernel-Matrix $H$ bilden ohne explizit $F$ zu konstruieren. Jedoch wird es auch bei dieser Methode schwieriger, sobald die Dimension sehr hoch ist.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3. PCA und K-Means Clustering\n",
    "## Motivation & Mehrwert\n",
    "Für hochdimensionale Trainingsdaten kann es beim Trainingsprozess und der späteren Anwendung des Modells in einer Laufzeitumgebung zu Engpässen im Hinblick auf die gegebenen Ressourcen kommen. Daher ist es ratsam,<b> bereits vor dem Trainingsprozess</b> Reduktionsstrategien sowohl im Hinblick auf die Feature-Dimension als auch auf die Anzahl der Datenpunkte zu nutzen.<br>\n",
    "<br>\n",
    "Ein weiterer Gesichtspunkt ist, dass die Reduktion der Dimension die Visualisierung zugrundeliegender Daten ermöglichen kann, was stark zum Verständnis der Daten und des Problems beitragen kann.\n",
    "## Schematische Darstellung\n",
    "Zur Reduktion der Anzahl Datenpunkte werden wir den <b>K-Means-Algorithmus</b> und das damit Verbundene Clustering vorstellen.<br>\n",
    "Zur Reduktion der Dimension des Feature-Raums stellen wir die <b>Primary Component Analysis (PCA)</b> vor.<br><br>\n",
    "Die folgende Darstellung zeigt die Reduktionsrichtungen für einen Basisdatensatz der Dimension $P\\times N$:\n",
    "<img src=\"dimred.jpg\">\n",
    "## Ziel des K-Means Clustering\n",
    "Es sei ein Dataset $\\{x_p\\}_{p=1}^P$ gegeben mit $x_p\\in\\mathbb{R}^N$<br>\n",
    "Wähle $K\\in\\mathbb{N}$ beliebig (ggf. getrieben durch Vorwissen) aber fest.<br><br>\n",
    "<b>Ziel</b>: Finde eine \"gute\" Zuordnung $x_p\\sim S_i$ für alle $x_p$ und $i\\in\\{1,\\dots,K\\}$<br><br>\n",
    "Wir sagen, dass die Zuordnung gut ist, wenn alle Datenpunkte $x_p$ mit $x_p\\sim Q_i$ sich nach zuvor festgelegten Kriterien möglichst ähnlich sind.<br><br>\n",
    "Aus Gründen der Anschaulichkeit wählen wir im Folgenden \"räumliche Nähe\" als Kriterium, d.i. ein kleiner Wert für den euklidischen Abstand für alle Datenpunkte innerhalb eines Clusters. Das Vorgehen lässt sich ohne größere Schwierigkeiten auf andere Kriterien übertragen.<br><br>\n",
    "## Ziel der PCA\n",
    "Motivation: Finde einen Unterraum der Dimension $K<N$ und Gewichtesvektoren $w_p \\in \\mathbb{R}^K$ für jedes $x_p$, sodass wir ein jedes $x_p$ durch $K<N$ Koordinaten beschreiben können und diese trotzdem eine <i>starke</i> Repräsentation der Daten ergeben.\n",
    "## Beispiel: Beste Position von Fussballspielern\n",
    "Gegeben ist ein Datensatzes aus dem Videospiel FIFA 2018 (Quelle: Kaggle), bei dem etwa 17500 Profifußballer jeweils durch einen Datenpunkt beschrieben werden. Beschreibende Features sind z.B. Größe, Gewicht, Ausdauer, Antritt, Passspiel, Schussstärke, Flanken, Dribbeln, ...<br>\n",
    "<br>\n",
    "<b>Ziel:</b> Der Datensatz soll genutzt werden, um ein Klassifikationsmodell zu trainieren. Dieses soll einem neuen Spieler diejenige Position (Torwart, Verteidigung, Mittelfeld, Sturm) zuweisen, auf der der Spieler die beste Leistung zeigen wird.<br>\n",
    "<br>\n",
    "Der unten dargestellte Scatter-Plot zeigt ein 4-Means-Clustering auf den Datensatz, nachdem dieser durch drei extrahierte PCA-Features reduziert wurde:<br>\n",
    "<img src=\"KMeans_modif.png\" width=\"75%\">\n",
    "<br>\n",
    "$\\rightarrow$Vergleicht man dieses Ergebnis mit den Labels des Datensatzes (das sind die tatsächlich besten Spielfeldpositionen), so sieht man sehr schön, dass das K-Means-Clustering mit Cluster 1 gerade die Torhüter gefunden hat (das Modell, das trainiert wurde war tatsächlich in der Lage, Torhüter mit 100% Genauigkeit zu klassifizieren!). Somit können wir bereits im Vorhinein alle Punkte, die in Cluster 1 fallen mit dem Label \"Torhüter\" versehen und so die Effizienz der Vorhersagengewinnung stark reduzieren, da hier der Rechenaufwand durch die ermittelte Entscheidungsfläche durch einen einfach Distanzberechnung zum Zentrum von Cluster 1 ersetzt werden kann. <br>\n",
    "<br>\n",
    "$\\rightarrow$ Das Clustering gibt zudem bereits Aufschluss darüber, dass es bei den Datenpunkten der restlichen Klassen zu relativ starken Durchmischungen kommt (Ränder der Cluster berühren sich), was die Treffergenauigkeit der Klassifizierung verschlechtern wird. Tatsächlich stellt sich später heraus, dass die Klassifizierung des Mittelfelds am schlechtesten ausfällt (was eigentlich auch zu erwarten war).\n",
    "<br><br>\n",
    "\n",
    "Zuallerletzt sei hier noch das erzielte Scoring eines Support-Vector-Classifiers (SVC) mit dem Kernel der Radialen Basisfunktionen (rbf) und Schlupfkonstante (Slack) C=10 angegeben:<br>\n",
    "<center><img src=\"score_svc_rbf.PNG\" width=\"40%\" style=\"border:1px solid black\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Der K-Means-Algorithmus\n",
    "Es sei ein Datensatz $\\{x_p\\}_{p=1}^P$ gegeben mit $x_p\\in\\mathbb{R}^N$.<br>\n",
    "Wähle $K\\in\\mathbb{N}$ beliebig (ggf. getrieben durch Vorwissen wie im Beispiel) aber fest.<br><br>\n",
    "## Definition: Cluster $S_i$ und Clustering $\\bigcup_{i=1}^{K}S_i$\n",
    "Ein Cluster $S_i$ ist eine Indexmenge $S_i\\subset\\{1,\\dots,P\\}$<br><br>\n",
    "Für jeweils zwei Cluster $S_i,S_j$ mit $i\\neq j$ soll $S_i \\cap S_j=\\emptyset$ gelten.<br><br>\n",
    "Wir bezeichnen  $Q_D=\\bigcup_{i=1}^{K}S_i$ als vollständiges Clustering für einen Datensatz $D=\\{x_p\\}_{p=1}^P$, <br>\n",
    "wenn die $S_i$ Cluster sind und $Q_D=\\{1,\\dots,P\\}$ gilt.    \n",
    "<br>\n",
    "## Problemstellung\n",
    "<b>Ziel:</b> Finde für jedes Cluster $S_i$ einen Zentroiden $c_i, i\\in\\{1,\\dots,K\\}$, so dass<br><br>\n",
    "$$c_k\\approx x_p $$\n",
    "$$ \\forall p\\in S_k \\space \\text{und}\\space k=1,\\dots,K$$<br><br>\n",
    "## Generalisierung\n",
    "Fromuliere dieses Problem nun um zu<br>\n",
    "$$Ce_k \\approx x_p \\space \\forall p \\in S_k$$ <br>\n",
    "wobei $C=[c_1 \\space c_2 \\dots c_K]$<br><br><br>\n",
    "Mit der Datenmatrix $X=[x_1 \\space x_2 \\dots x_p]$ und der Assoziationsmatrix $W=[w_1 \\space w_2 \\dots w_P]$,<br> wobei $w_p = e_k$, wenn $p \\in S_k$ schreibt man nun noch vereinfachter<br><br>\n",
    "$$CW \\approx X$$\n",
    "<br><br>\n",
    "## Formulierung eines Optimierungsproblems\n",
    "Mit der schreibweise von oben erhält man ein erstes handliches und lösbaes Optimierungsproblem, nämlich<br>\n",
    "$$\\min_{C,W}\\|CW-X\\|_F^2$$\n",
    "$$u.d.N.\\space w_p\\in\\{e_k\\}_{k=1}^K, \\space p=1,\\dots,P$$\n",
    "<br>\n",
    "<b>Wichtige Notiz:</b> Dieses Optimierungsproblem ist nicht-konvex und muss zudem getrennt und alternierend nach $C$ und $W$ optimiert werden. <br>\n",
    "Siehe dazu den folgenden Algorithmus, der dies berücksichtigt:\n",
    "## K-Means-Algorithmus\n",
    "<div style=\"background:lightgrey; border:1px solid grey;font-family:garamond; padding:30px;width:60%\">\n",
    "<b>Input:</b><br> \n",
    "Datenmatrix $X$, Zentroidenmatrix $C$ (zufällig intialisiert)<br>\n",
    "Assoziationsmatrix $W$ (initialisert durch Nullmatrix)<br>\n",
    "<b>Output:</b><br>\n",
    "Optimale Zentroidenmatrix $C^*$ und optimale Assoziationsmatrix $W^*$<br>\n",
    "<b>Algorithmus:</b><br>\n",
    "(1) $W$ für alle $p=1,\\dots,P$ anpassen: $w_p=e_{k^*}$, wobei $k^*=argmin_{k=1,\\dots,K}\\|c_k-x_p\\|_2^2$<br>\n",
    "(2) $C$ für alle $k=1,\\dots,K$ anpassen: $S_k$ updaten und $c_k=\\frac{1}{|S_k|}\\sum_{p\\in S_k}x_p$ setzen.<br>\n",
    "<i>Wiederhole (1) und (2) bis sich Konvergenz einstellt.</i>\n",
    "</div>\n",
    "## TBD: Verbesserung des K-Means-Algorithmus' (S.250)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Principal component analysis (PCA)\n",
    "Motivation: Finde einen Unterraum der Dimension $K<N$ und Gewichtesvektoren $w_p \\in \\mathbb{R}^K$ für jedes $x_p$, sodass wir ein jedes $x_p$ durch $K<N$ Koordinaten beschreiben können.\n",
    "\n",
    "Wir suchen Vektoren $c_1, \\dots, c_K$, die unsere $N$-dimensionalen Daten $x_1,\\dots , x_P$ gut darstellt, wobei $K\\leq N$.Wir projizieren unsere Datenpunkte $x_p$ für alle $p=1,\\dots, P$ in einen Unterraum:\n",
    " <img src=PCAc.jpg> <br>\n",
    "Formalisiert bedeutet dies also: <br>\n",
    "Für jedes $p=1,\\dots, P$ schreibe<br> \n",
    "$$\\sum_{k=1}^K c_k w_{k,p} \\approx x_p$$<br>\n",
    "mit unseren Gewichten $w_{k,p} \\in \\mathbb{R}$. <br>\n",
    "<br>\n",
    "<br>\n",
    "Wir speichern nun die Vektoren spaltenweise in eine $N\\times K$ Matrix $C$ und die $w_{k,p}$ in einem $K\\times 1$ Vektor $w_p$ und schreiben:<br><br>\n",
    "$$C w_p \\approx x_p$$ \n",
    "für jedes $p=1,\\dots , P$.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Wir speichern weiterhin die Gewichte über die $x_p$ in eine $K\\times P$ Matrix $W=[w_1|\\cdots |w_P]$ und unsere Daten in eine $N\\times P$ Matrix $X=[x_1|x_2|\\cdots |x_P]$, also:<br><br>\n",
    "$$CW\\approx X$$ \n",
    "<br><br><br>\n",
    "Das Ziel ist es nun $\\|CW-X \\|_F^2$ über $C$ und $W$ zu minimieren:<br><br>\n",
    "$$\\displaystyle \\min_{C,W} \\|CW-X \\|_F^2$$<br>\n",
    "um den optimalen Unterraum zu finden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
