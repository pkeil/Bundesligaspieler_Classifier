{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "1. Backpropagation\n",
    "    - Wiederholung Feed Forward Neural Network mit Parametern $M_1,M_2,...,M_k$\n",
    "    - Generalisierte Darstellung\n",
    "    - Backpropagation-Algorithmus\n",
    "2. K-means Clustering\n",
    "    - Motivation\n",
    "    - Minimierungsproblem\n",
    "    - Anwendungsfälle\n",
    "# Backpropagation:\n",
    "## Wiederholung\n",
    "Zur Regression bzw. Klassifizierung verwenden wir nun parametrisierbare Basiselemente und fügen diese durch Summation und Komposition zu einem Netzwerk zusammen, dessen Parameter wir auf den Zielvektor $y\\in\\mathbb{R}^N$ hin optimieren möchten.\n",
    "### Definition: Feed Forward Neural Network mit Parametern $M_1, M_2, ...,M_k$\n",
    "#### Voraussetzungen:\n",
    "<br>\n",
    "<div style=\"border:1px solid lightgrey;padding:20px;\">\n",
    "Daten $\\{x_p,y_p\\}_{p=1}^P$ mit $x_p\\in\\mathbb{R}^N,y\\in\\mathbb{R}$<br><br>\n",
    "$M_1,M_2,\\dots,M_k$, wobei $M_i$ die Anzahl der Neuronen des $i$-ten Hidden Layers angibt<br><br>\n",
    "$k$ bezeichnet die Gesamtanzahl an Hidden Layern <br><br>\n",
    "$f:\\mathbb{R}^N\\rightarrow \\mathbb{R}$ eine Basisfunktion <br>(z.b. $f(x)=tanh(c_m+x^Tv_m)$ mit Parametern $c_m\\in\\mathbb{R}, v_m\\in\\mathbb{R}^N$)<br><br>\n",
    "Aktivierungsfunktion $a:\\mathbb{R}\\rightarrow\\mathbb{R}$ \n",
    "<br>(z.b. $a(\\centerdot)=tanh(\\centerdot)$)\n",
    "</div>\n",
    "<br><br>\n",
    "Folgendes Bild zeigt die Konstruktion von Basisfeatures $f_m$, mit Hilfe derer man $r=\\sum_{m=1}^{M}w_mf_m$ konstruiert: <br><br>\n",
    "<img src=\"FeedForwardNeuralNetwork.jpg\">\n",
    "\n",
    "## Über die Konstruktion eines Basiselements $f_m$ zur Zielfunktion $r$ \n",
    "Sei $m\\in\\{1,\\dots,M\\}$ beliebig aber fest.<br>\n",
    "Es seien $c^{(2)}=(c_{1}^{(2)},\\dots,c_{m_2}^{(2)},\\dots,c_{M}^{(2)})^T\\in\\mathbb{R}^{M_2\\times1}$ sowie <br>$c^{(1)}=(c_{1}^{(1)},\\dots,c_{m}^{(1)},\\dots,c_{M_1}^{(1)})^T\\in\\mathbb{R}^{M_1\\times1}$ und <br>\n",
    "$V=???$\n",
    "<br><br>\n",
    "Dann kann man schreiben:<br>\n",
    "$$f_m(x_p)=a(c_m^{(1)}+\\sum_{m_2=1}^{M_2}[a(c_{m_2}^{(2)}+\\sum_{n=1}^Nx_{p,n}v_{n,m_2}^{(2)})v_{m_2,m}^{(1)}]$$\n",
    "<br><br>\n",
    "Und noch kompakter in Matrixschreibweise für alle $m=1,\\dots,M_1$:<br>\n",
    "$ f_p=a(c^{(1)}+V^{T(1)}a(c^{(2)}+V^{T(2)}x_p))$, wobei $a(\\centerdot)$ komponentenweise operiert.\n",
    "<br><br>\n",
    "Somit erhält man für $r=\\sum_{m=1}^{M_1}w_mf_m=b+w^Ta[c^{(1)}+V^{T(1)}a(c^{(2)}+V^{T(2)}x_p)]$ das Rekursionsmuster<br><br>\n",
    "0. $r^{(0)}=b+w^Ta(r^{(1)})$\n",
    "1. $r^{(1)}=c^{(1)}+V^{T(1)}a(r^{(2)})$\n",
    "2. $r^{(2)}=c^{(2)}+V^{T(2)}x_p$\n",
    "<br><br>\n",
    "Im Folgenden werden wir dieses Rekursionsmuster in Kombination mit einer Kostenfunktion $h(\\centerdot)$ ausnutzen, um den Gradienten für das Gradientenverfahren angeben zu können. Dieses Vorgehen bezeichnet man als Backpropagation-Algorithmus:\n",
    "## Backpropagation-Algorithmus\n",
    "Die Zielsetzung besteht darin, eine Kostenfunktion $h(r)$ mit dem Gradientenverfahren minimieren zu können. Dabei ist folgende Verkettungsfolge gegeben:<br>\n",
    "$h(r)\\rightarrow r(b,w) \\rightarrow [b(c^{(1)},V^{(1)}),w(c^{(1)},V^{(1)})]\\rightarrow [c^{(1)}(c^{(2)},V^{(2)}),V^{(1)}(c^{(2)},V^{(2)})]\\rightarrow[c^{(2)}(x_p),V^{(2)}(x_p)]$\n",
    "\n",
    "<br><br><br><br>\n",
    "Übliche Funktionsscharen für die Basiselemente sind<br>\n",
    "1. $b_m(x)=tanh(c_m+x_mv_m)$ mit Aktivierungsfunktion $a(.)=tanh(.)$<br>\n",
    "2. $b_m(x)=max(0,c_m+x^Tv_m)$ mit Aktivierungsfunktion $a(.)=max(0,.)$\n",
    "\n",
    "<br><br><br>\n",
    "# PCA und K-Means Clustering\n",
    "## Motivation K-Means Clustering\n",
    "Es sei ein Dataset $\\{x_p\\}_{p=1}^P$ gegeben mit $x_p\\in\\mathbb{R}^N$<br>\n",
    "Wähle $K\\in\\mathbb{N}$ beliebig (ggf. getrieben durch Vorwissen) aber fest.<br><br>\n",
    "<b>Ziel</b>: Finde eine \"gute\" Zuordnung $x_p\\sim S_i$ für alle $x_p$ und $i\\in\\{0,\\dots,K-1\\}$<br><br>\n",
    "Wir sagen, dass die Zuordnung gut ist, wenn alle Datenpunkte $x_p$ mit $x_p\\sim Q_i$ sich nach zuvor festgelegten Kriterien möglichst ähnlich sind.<br><br>\n",
    "Aus Gründen der Anschaulichkeit wählen wir im Folgenden \"räumliche Nähe\" als Kriterium, d.i. ein kleiner Wert für den euklidischen Abstand für alle Datenpunkte innerhalb eines Clusters. Das Vorgehen lässt sich ohne größere Schwierigkeiten für auf andere Kriterien übertragen.<br><br>\n",
    "## Beispiel: Beste Position von Fussballspielern\n",
    "\n",
    "X: Standardisierter Datensatz mit 17500 Profifußballern, jeweils beschrieben durch 185 Features,<br><br>\n",
    "Y: \"Beste Spielfeldposition\": Torwart, Verteidigung, Mittelfeld, Sturm (Vorwissen: 4 Klassen)<br><br>\n",
    "Reduktion der Features auf 3 PCA-Features und 4-Means-Clustering.\n",
    "### Ergebnis 4-Means Clustering (unsupervised)\n",
    "<img src=\"KMeans_modif.png\">\n",
    "### Die Realität (supervised)\n",
    "<img src='Means_modif.jpg'>\n",
    "### Analyse\n",
    "\n",
    "<img src=\"KMeans_comp.png\">\n",
    "Das 4-Means-Clustering ordnet erkennt die Klassen 'Torwart' und 'Sturm' relativ gut.\n",
    "<img src=\"K_Means_mixareas.png\">\n",
    "In einigen Bereichen gibt es starke Durchmischung der Datenpunkte.\n",
    "<img src=\"comp_kmeans_stats.png\">\n",
    "\n",
    "### Schlussfolgerung:\n",
    "Wir können Datenpunkte der Klasse 'Torwart' ohne größere Bedenken auf den zugehörigen Zentroiden abbilden und so die Dimension des Datensatzes reduzieren. In den anderen Fällen verschlechtert die Dimensionsreduktion die Vorhersagegenauigkeit.\n",
    "\n",
    "### Der K-Means-Algorithmus\n",
    "Es sei ein Datensatz $\\{x_p\\}_{p=1}^P$ gegeben mit $x_p\\in\\mathbb{R}^N$<br>\n",
    "Wähle $K\\in\\mathbb{N}$ beliebig (ggf. getrieben durch Vorwissen) aber fest.<br><br>\n",
    "#### Def. Cluster $S_i$ und Clustering\n",
    "Ein Cluster $S_i$ ist eine Indexmenge $S_i\\subset\\{1,\\dots,P\\}$<br><br>\n",
    "Für jeweils zwei Cluster $S_i,S_j$ mit $i\\neq j$ gilt zudem $S_i \\cap S_j=\\O$\n",
    "<b>Ziel:</b> Finde für jedes Cluster $S_i$ einen Zentroiden $c_i, i\\in\\{0,\\dots,K-1\\}$, so dass<br>\n",
    "$c_k\\approx x_p$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Der K-Means-Algorithmus\n",
    "Es sei ein Datensatz $\\{x_p\\}_{p=1}^P$ gegeben mit $x_p\\in\\mathbb{R}^N$<br>\n",
    "Wähle $K\\in\\mathbb{N}$ beliebig (ggf. getrieben durch Vorwissen) aber fest.<br><br>\n",
    "#### Def. Cluster $S_i$ und Clustering $\\bigcup_{i=1}^{K}S_i$\n",
    "Ein Cluster $S_i$ ist eine Indexmenge $S_i\\subset\\{1,\\dots,P\\}$<br><br>\n",
    "Für jeweils zwei Cluster $S_i,S_j$ mit $i\\neq j$ gilt zudem $S_i \\cap S_j=\\emptyset$<br>\n",
    "Wir bezeichnen  $Q_D=\\bigcup_{i=1}^{K}S_i$ als vollständiges Clustering für einen Datensatz $D=\\{x_p\\}_{p=1}^P$, <br>\n",
    "wenn die $S_i$ Cluster sind und $Q_D=\\{1,\\dots,P\\}$ gilt.    \n",
    "<br>\n",
    "#### Problemstellung\n",
    "<b>Ziel:</b> Finde für jedes Cluster $S_i$ einen Zentroiden $c_i, i\\in\\{1,\\dots,K\\}$, so dass<br>\n",
    "$c_k\\approx x_p$ $\\forall p\\in S_k$ und $k=1,\\dots,K$<br><br>\n",
    "Fromuliere dieses Problem um zu<br>\n",
    "$$Ce_k \\approx x_p \\space \\forall p \\in S_k$$ <br>\n",
    "mit $C=[c_1 \\space c_2 \\dots c_K]$<br><br>\n",
    "Mit der Datenmatrix $X=[x_1 \\space x_2 \\dots x_p]$ und der Assoziationsmatrix $W=[w_1 \\space w_2 \\dots w_P]$,<br> wobei $w_p = e_k$, wenn $p \\in S_k$ schreibt man nun noch vereinfachter<br>\n",
    "$$CW \\approx X$$\n",
    "<br><br>\n",
    "#### Erste Formulierung eines Optimierungsproblems\n",
    "Mit der schreibweise von oben erhält man ein erstes handliches und lösbaes Optimierungsproblem, nämlich<br>\n",
    "$$\\min_{C,W}\\|CW-X\\|_F^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Konstruktion eines  $(k,(M_1,M_2,...,M_n))$ - feed forward Neural Networks\n",
    "Recap: Es geht darum, eine geeignete Funktion $f = (f_1,f_2,...,f_N):\\mathbb{R}^N\\rightarrow\\mathbb{R}$ zu finden. Wir möchten nun auf Basis der Basiselemente $b_m$ Basis-Features (eines für jedes $F_i$) erzeugen, deren Konstruktion durch $k$ und $(M_1,M_2,...,M_n)$ festgelegt wird. Es wird nach folgendem Schema konstruiert:\n",
    "\n",
    "$f_m(x) = a( \\sum_{m_1=1}^{M_1}b_1(\\sum_{m_2=1}^{M_2}b_2(\\sum_{m_3=1}^{M_3}b_3(\\sum_{m_4=1}^{M_4}(\\dots( \\dots \\sum_{m_k=1}^{M_k}b_k))))\\dots)$<br>\n",
    "und das wäre beispielsweise für $b_m(x)=tanh(c_m+x^Tv_m)$ mit Aktivierungsfunktion $a(.)=tanh(.)$:<br>\n",
    "$f_m(x) = tanh( \\sum_{m_1=1}^{M_1}c_{m_1}+(\\sum_{m_2=1}^{M_2}a(\\sum_{m_3=1}^{M_3}b_2(\\sum_{m_4=1}^{M_4}\\dots)$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
